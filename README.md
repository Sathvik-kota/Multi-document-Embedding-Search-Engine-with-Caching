---
title: Document Search Engine
emoji: ğŸ“„
colorFrom: blue
colorTo: purple
sdk: docker
sdk_version: "0.0.0"
app_file: start.sh
pinned: false
---

#  Multi-Document Semantic Search Engine  
A **production-inspired multi-microservice semantic search system** built over 20+ text documents.

Designed with:

- **Sentence-Transformers** (`all-MiniLM-L6-v2`)
- **Local Embedding Cache**
- **FAISS Vector Search + Persistent Storage**
- **LLM-Driven Explanations (Gemini 2.5 Flash)**
- **Google-Gemini-Style Streamlit UI**
- **Real Microservice Architecture**
- **Full Evaluation Suite (Accuracy Â· MRR Â· nDCG)**

A complete end-to-end ML system demonstrating real-world architecture & search engineering.

---

#  Features

## ğŸ”¹ Core Search

- Embedding-based semantic search over `.txt` documents  
- FAISS `IndexFlatL2` on **normalized vectors** (â‰ˆ cosine similarity)  
- Top-K ranking + similarity scores  
- Keyword overlap, overlap ratio  
- Top semantic sentences  
- Full-text preview  

---

## ğŸ”¹ Microservice Architecture (5 FastAPI Services)

Each component runs as an **independent microservice**, mirroring real production systems:

| Service | Responsibility |
|--------|----------------|
| **doc_service** | Load, clean, normalize, hash, and store documents |
| **embed_service** | MiniLM embedding generation + caching |
| **search_service** | FAISS index build, update, and vector search |
| **explain_service** | Keyword overlap, top sentences, LLM explanations |
| **api_gateway** | Orchestration: a clean unified API for the UI |
| **streamlit_ui** | Gemini-style user interface |

This separation supports **scalability**, **fault isolation**, and **independent service upgrades** â€” *like real enterprise ML platforms*.

---

## ğŸ”¹ Explanations

Every search result includes:

- **Keyword overlap**
- **Semantic overlap ratio**
- **Top relevant sentences (MiniLM sentence similarity)**
- **LLM-generated explanation**:  
  â€œWhy did this document match your query?â€

---

## ğŸ”¹ Evaluation Suite
A built-in evaluation workflow providing:

- **Accuracy**
- **MRR (Mean Reciprocal Rank)**
- **nDCG@K**
- Correct vs Incorrect queries  
- Per-query detailed table  


---

#  How Caching Works (MANDATORY SECTION)

Caching happens inside **`embed_service/cache_manager.py`**.

### âœ” Zero repeated embeddings  
Each document is fingerprinted using:

- **filename**
- **MD5(cleaned_text)**

If the hash matches a previously stored file:
- cached embedding is loaded instantly  
- prevents costly re-embedding  
- improves startup & query latency  

### Cache Files:
- `cache/embed_meta.json` â†’ mapping of filename â†’ `{hash, index}`
- `cache/embeddings.npy` â†’ matrix of all embeddings

### Benefits
- Startup: **5â€“10 seconds â†’ <1 second**
- Low compute cost  
- Ideal for Hugging Face Spaces  
- Guarantees reproducible results  

---

#  FAISS Persistence (Warm Start Optimization)

This project saves BOTH embeddings and FAISS index:

- `cache/embeddings.npy`  
- `cache/embed_meta.json`  
- `faiss_index.bin`  
- `faiss_meta.pkl`  

On startup:search_service.indexer.try_load()


If found â†’ loaded instantly.  
If not â†’ FAISS index is rebuilt from cached embeddings.

### Why this matters?
- Makes FAISS behave like a **persistent vector database**  
- Extremely important for **Docker**, **Spaces**, and **cold restarts**  
- Zero delay in rebuilding large indexes  

---

#  Folder Structure 
```
â”œâ”€â”€ ğŸ“ src
â”‚ â”œâ”€â”€ ğŸ“ doc_service
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ app.py
â”‚ â”‚ â””â”€â”€ utils.py
â”‚ â”‚
â”‚ â”œâ”€â”€ ğŸ“ embed_service
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ app.py
â”‚ â”‚ â”œâ”€â”€ embedder.py
â”‚ â”‚ â””â”€â”€ cache_manager.py
â”‚ â”‚
â”‚ â”œâ”€â”€ ğŸ“ search_service
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ app.py
â”‚ â”‚ â””â”€â”€ indexer.py
â”‚ â”‚
â”‚ â”œâ”€â”€ ğŸ“ explain_service
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ app.py
â”‚ â”‚ â””â”€â”€ explainer.py
â”‚ â”‚
â”‚ â”œâ”€â”€ ğŸ“ api_gateway
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â””â”€â”€ app.py
â”‚ â”‚
â”‚ â””â”€â”€ ğŸ“ ui
â”‚ â””â”€â”€ streamlit_app.py
â”‚
â”œâ”€â”€ ğŸ“ data
â”‚ â””â”€â”€ ğŸ“ docs
â”‚ â””â”€â”€ (your .txt documents)
â”‚
â”œâ”€â”€ ğŸ“ cache
â”‚ â”œâ”€â”€ embed_meta.json
â”‚ â”œâ”€â”€ embeddings.npy
â”‚ â”œâ”€â”€ faiss_index.bin
â”‚ â””â”€â”€ faiss_meta.pkl
â”‚
â”œâ”€â”€ ğŸ“ eval
â”‚ â”œâ”€â”€ evaluate.py
â”‚ â””â”€â”€ generated_queries.json
â”‚
â”œâ”€â”€ start.sh
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

```
---

#  How to Run Embedding Generation

Embeddings generate automatically during initialization:


Pipeline:

1. **doc_service** â†’ load + clean + hash  
2. **embed_service** â†’ create or load cached embeddings  
3. **search_service** â†’ FAISS index build or load  
4. Return summary  


---

#  How to Start the API 

All services are launched using:

```bash
bash start.sh

This starts:

9001 â†’ doc_service

9002 â†’ embed_service

9003 â†’ search_service

9004 â†’ explain_service

8000 â†’ api_gateway

7860 â†’ Streamlit UI
```
---

##  Architecture Overview

### High-level Flow

1. User asks a question in **Streamlit UI**
2. UI sends request â†’ **API Gateway** `/search`
3. Gateway:
   - Embeds query via **Embed Service**
   - Searches FAISS via **Search Service**
   - Fetches full doc text from **Doc Service**
   - Gets explanation from **Explain Service**
4. Response returned to UI with:
   - filename, score, preview, full text
   - keyword overlap, overlap ratio
   - top matching sentences
   - optional LLM explanation



---

##  Design Choices

### 1ï¸âƒ£ **Microservices instead of Monolithic**
- Real-world ML systems separate **indexing, embedding, routing, and inference**.
- Enables **independent scaling**, easier debugging, and service-level isolation.


---

### 2ï¸âƒ£ **MiniLM Embeddings**
-  **Fast on CPU** (optimized for lightweight inference)
- **High semantic quality** for short & long text
- **Small model** â†’ ideal for search engines, mobile, Spaces deployments

---

### 3ï¸âƒ£ **FAISS L2 on Normalized Embeddings**
L2 distance is used instead of cosine because:

- **FAISS FlatL2 is faster** and more optimized
- When vectors are normalized:  
  `L2 Distance â‰¡ Cosine Distance` (mathematically equivalent)
-  Avoids the overhead of cosine kernels

---

### 4ï¸âƒ£ **Local Embedding Cache**
- Reduces startup time from **~5 seconds â†’ <1 second**
- Prevents **re-embedding identical documents**
-Allows FAISS persistence to work smoothly
- Speeds up startup & indexing
---
### 5ï¸âƒ£FAISS Persistence (Warm Start Optimization)
- Eliminates the need to rebuild index on each startup
- Warm-loads instantly at startup
- Ideal for Spaces & Docker environments
- A lightweight vector-database
---
### 6ï¸âƒ£ **LLM-Driven Explainability**
- Generates **human-friendly reasoning**. Makes search results more interpretable and intelligent.
- Explains **why a document matched your query**
- Combines:
  - Top semantic-matching sentences  
  - Keyword overlap  
  - Geminiâ€™s natural-language reasoning  

---

### 7ï¸âƒ£ **Streamlit for Fast UI**
-  Instant reload during development  
-  Clean layout 
- Easy to extend (evaluation panel, metrics, expanders)





